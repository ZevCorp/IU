# HRM Service for Jetson Orin Nano

Este servicio ejecuta el modelo **HRM (Hierarchical Reasoning Model)** en un Jetson Orin Nano Super Dev Kit, conectÃ¡ndose a un backend en Render para resolver laberintos en tiempo real.

## ğŸ”‘ Punto Clave: HRM NO es un paquete pip

**HRM no se puede instalar con `pip install`**. El repositorio no tiene `setup.py` ni `pyproject.toml`. 

La forma correcta de usarlo es:
1. **Clonar el repositorio**: `git clone https://github.com/sapientinc/HRM.git`
2. **AÃ±adir al PYTHONPATH**: `export PYTHONPATH="${PYTHONPATH}:$(pwd)/HRM"`
3. **Ejecutar el script** con el PYTHONPATH modificado

## ğŸ“¦ Estructura del Proyecto

```
jetson/
â”œâ”€â”€ HRM/                    # â† Repositorio HRM clonado (NO pip-instalado)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ hrm/
â”‚   â”‚       â””â”€â”€ hrm_act_v1.py
â”‚   â”œâ”€â”€ pretrain.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ hrm_service.py          # â† Nuestro servicio WebSocket
â”œâ”€â”€ run_hrm_service.sh      # â† Script que configura PYTHONPATH
â”œâ”€â”€ setup.sh                # â† Setup completo
â”œâ”€â”€ requirements.txt        # â† Dependencias adicionales
â””â”€â”€ README.md               # â† Este archivo
```

## ğŸš€ InstalaciÃ³n RÃ¡pida

```bash
# En la Jetson:
cd ~/IU/jetson

# Ejecutar setup (hace todo automÃ¡ticamente)
chmod +x setup.sh
./setup.sh
```

## ğŸ“‹ InstalaciÃ³n Manual

Si prefieres hacer todo manualmente:

```bash
# 1. Instalar PyTorch para Jetson (con CUDA)
pip3 install torch torchvision --extra-index-url https://developer.download.nvidia.com/compute/pytorch/whl/cu118

# 2. Clonar HRM (NO pip install)
git clone --recursive https://github.com/sapientinc/HRM.git

# 3. Instalar dependencias de HRM
pip3 install -r HRM/requirements.txt

# 4. Instalar dependencias del servicio
pip3 install -r requirements.txt

# 5. Configurar PYTHONPATH y ejecutar
export PYTHONPATH="${PYTHONPATH}:$(pwd)/HRM"
python3 hrm_service.py --server wss://iu-rw9m.onrender.com
```

## â–¶ï¸ EjecuciÃ³n

### OpciÃ³n 1: Usando el script (Recomendado)

```bash
./run_hrm_service.sh --server wss://iu-rw9m.onrender.com
```

### OpciÃ³n 2: Manual con PYTHONPATH

```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd)/HRM"
python3 hrm_service.py --server wss://iu-rw9m.onrender.com
```

### Opciones disponibles

| OpciÃ³n | DescripciÃ³n |
|--------|-------------|
| `--server URL` | URL del servidor WebSocket |
| `--model ID` | ID del modelo en HuggingFace (default: `sapientinc/HRM-checkpoint-maze-30x30-hard`) |
| `--hrm-path PATH` | Ruta al repositorio HRM |
| `--bfs-only` | Usar solo BFS (sin cargar HRM) |
| `--test` | Solo probar conexiÃ³n |

## ğŸ§  CÃ³mo Funciona HRM

1. **Descarga de Checkpoint**: El modelo se descarga automÃ¡ticamente de HuggingFace:
   - `sapientinc/HRM-checkpoint-maze-30x30-hard` (~109MB)
   - Se guarda en `~/.cache/hrm/`

2. **Arquitectura**: HRM usa una arquitectura de razonamiento jerÃ¡rquico con:
   - Dos niveles de razonamiento (H-level y L-level)
   - Adaptive Computation Time (ACT) para decidir cuÃ¡ndo parar
   - ~27M parÃ¡metros

3. **Inferencia**: Para cada laberinto:
   - Recibe grid como tokens (0=wall, 1=path, 2=start, 3=target)
   - Ejecuta ciclos de razonamiento hasta convergencia
   - Retorna el camino Ã³ptimo

4. **Fallback**: Si HRM falla, usa BFS (Breadth-First Search) como respaldo.

## ğŸ“Š Checkpoints Disponibles

| Modelo | DescripciÃ³n | HuggingFace ID |
|--------|-------------|----------------|
| Maze 30x30 Hard | Laberintos 30x30 difÃ­ciles | `sapientinc/HRM-checkpoint-maze-30x30-hard` |
| Sudoku Extreme | Sudoku nivel extremo | `sapientinc/HRM-checkpoint-sudoku-extreme` |
| ARC-AGI-2 | Razonamiento abstracto | `sapientinc/HRM-checkpoint-ARC-2` |

## ğŸ”§ Troubleshooting

### Error: "No module named 'pretrain'"
```bash
# HRM no estÃ¡ en PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)/HRM"
```

### Error: "CUDA not available"
```bash
# Verificar instalaciÃ³n de PyTorch para Jetson
python3 -c "import torch; print(torch.cuda.is_available())"
```

### Error: "neither 'setup.py' nor 'pyproject.toml' found"
Este error aparece si intentas `pip install -e ./HRM`. **NO hagas esto**. HRM no es un paquete pip. Solo clÃ³nalo y usa PYTHONPATH.

### Error: "Failed to load model: ..."
El servicio caerÃ¡ automÃ¡ticamente a BFS. Los logs mostrarÃ¡n `[BFS]` en vez de `[HRM]` para cada solicitud.

## ğŸ“ Logs

```
2026-01-18 11:20:40 [INFO] Server: wss://iu-rw9m.onrender.com
2026-01-18 11:20:40 [INFO] Model: sapientinc/HRM-checkpoint-maze-30x30-hard
2026-01-18 11:20:43 [INFO] Using CUDA: Orin Nano (8.0 GB)
2026-01-18 11:21:15 [INFO] âœ… HRM Model loaded! Parameters: 27,345,678 (~27.3M)
2026-01-18 11:21:36 [INFO] Connected to wss://iu-rw9m.onrender.com
2026-01-18 11:21:40 [INFO] Inference (HRM) completed in 42.15ms, path length: 89
```

## ğŸ”— Referencias

- [HRM GitHub Repository](https://github.com/sapientinc/HRM)
- [HRM Paper (arXiv)](https://arxiv.org/abs/2506.21734)
- [Checkpoints on HuggingFace](https://huggingface.co/sapientinc)
